{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860c3cdc-11dd-40ab-9d33-88dbda68f95d",
   "metadata": {},
   "source": [
    "# Transformer-Based Reinforcement Learning: Theoretical \n",
    "\n",
    "This notebook explains how a Transformer encoder is used inside a DQN model for financial trading, with detailed mathematical formulations using LaTeX.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Input Representation\n",
    "Given a sequence of historical prices:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = [x_1, x_2, \\dots, x_T] \\in \\mathbb{R}^{T \\times 1}\n",
    "$$\n",
    "\n",
    "We project the prices to a higher-dimensional embedding space:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}^{(0)} = \\mathbf{X} \\cdot \\mathbf{W}_{\\text{input}} \\in \\mathbb{R}^{T \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Positional Encoding\n",
    "Add time-awareness to embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{\\text{pos}} = \\mathbf{H}^{(0)} + \\mathbf{P} \\quad \\text{(with } \\mathbf{P} \\in \\mathbb{R}^{T \\times d}\\text{)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Self-Attention\n",
    "Compute Query, Key, Value matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{H}_{\\text{pos}} \\cdot \\mathbf{W}^Q, \\quad\n",
    "\\mathbf{K} = \\mathbf{H}_{\\text{pos}} \\cdot \\mathbf{W}^K, \\quad\n",
    "\\mathbf{V} = \\mathbf{H}_{\\text{pos}} \\cdot \\mathbf{W}^V\n",
    "$$\n",
    "\n",
    "Then compute attention:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d}} \\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feedforward Layer\n",
    "Apply a position-wise feedforward network:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, x \\mathbf{W}_1 + b_1) \\mathbf{W}_2 + b_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Residual and Layer Normalization\n",
    "Each Transformer block includes:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{Attention}(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{FFN}(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Output for RL\n",
    "Take the output of the last token (most recent time step):\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{\\text{last}} = \\mathbf{H}_{\\text{pos}}^{(L)}[T]\n",
    "$$\n",
    "\n",
    "Then concatenate with auxiliary wallet features:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{\\text{final}} = [\\mathbf{z}_{\\text{last}}, c, k] \\in \\mathbb{R}^{d + 2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $c$ is normalized cash\n",
    "- $k$ is normalized crypto\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Q-Value Output\n",
    "Predict Q-values for each action:\n",
    "\n",
    "$$\n",
    "\\mathbf{q} = \\mathbf{z}_{\\text{final}} \\cdot \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}} \\in \\mathbb{R}^{9}\n",
    "$$\n",
    "\n",
    "Each entry in $\\mathbf{q}$ is the estimated value of taking one of the 9 possible actions:\n",
    "- Hold, Buy 25%‚Äì100%, Sell 25%‚Äì100%.\n",
    "\n",
    "---\n",
    "\n",
    "This architecture allows the agent to focus attention on critical past price points while factoring in portfolio state (cash/crypto) when making trade decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a9ddac8-2149-4e23-93c0-4ce26f5c1f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /gpfs/fs001/cbica/home/dadashkj/decipher\n",
      "üìÅ Checkpoint directory already exists: amexyz/checkpoints_120\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler for consistent scaling\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler for consistent scaling\n",
    "\n",
    "# --- Configuration Parameters ---\n",
    "WINDOW_SIZE = 120\n",
    "INITIAL_CASH = 10000\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "# Checkpoint directory now includes WINDOW_SIZE to differentiate models\n",
    "CHECKPOINT_DIR = \"amexyz/checkpoints_\" + str(WINDOW_SIZE)\n",
    "CHECKPOINT_INTERVAL = 10 # Save every 10 episodes for demonstration, you can change to 100\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "    print(f\"üìÇ Created checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    print(f\"üìÅ Checkpoint directory already exists: {CHECKPOINT_DIR}\")\n",
    "\n",
    "\n",
    "# 1. Data Acquisition\n",
    "def get_price_data(filename=\"btc_prices.csv\"):\n",
    "    \"\"\"\n",
    "    Downloads or loads Bitcoin (BTC-USD) historical 'Close' price data.\n",
    "    If the file exists, it loads from there; otherwise, it downloads from Yahoo Finance.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"üìÅ Loading BTC data from {filename}\")\n",
    "        try:\n",
    "            df = pd.read_csv(filename, index_col=0, parse_dates=True, skiprows=[1, 2])\n",
    "            if 'Close' not in df.columns:\n",
    "                print(\"Warning: 'Close' column not found after skipping rows. Trying without skipping.\")\n",
    "                df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename} with skiprows: {e}. Trying without skipping rows.\")\n",
    "            df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "\n",
    "        if 'Close' not in df.columns:\n",
    "            raise ValueError(f\"Error: 'Close' column not found in {filename}. Please check the CSV file.\")\n",
    "\n",
    "        df = df[['Close']]\n",
    "        df = df.dropna()\n",
    "        print(f\"Loaded data shape: {df.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(\"üåê Downloading BTC data from Yahoo Finance...\")\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        df = yf.download(\"BTC-USD\", start=\"2018-01-01\", end=end_date)\n",
    "        df.to_csv(filename)\n",
    "        print(f\"‚úÖ Data saved to {filename}\")\n",
    "\n",
    "    return df['Close'].values.astype(np.float32)\n",
    "\n",
    "# --- Positional Encoding for Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (seq_len, batch_size, feature_dim)\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# 2. Define Transformer-based DQN Model\n",
    "class TransformerDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Deep Q-Network (DQN) model for action prediction.\n",
    "    Input: state (sequence of past prices + current normalized cash + current normalized crypto)\n",
    "    Output: Q-values for each action (Hold, Buy 25%, ..., Sell 100%)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, nhead, num_encoder_layers, dim_feedforward, output_dim=9):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None # Not used in this setup\n",
    "        self.pos_encoder = PositionalEncoding(feature_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(feature_dim, nhead, dim_feedforward, dropout=0.1, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "\n",
    "        # The decoder needs to take the combined output of the transformer and the auxiliary features\n",
    "        # The transformer outputs `feature_dim` (which is 1 in your current setup for price)\n",
    "        # The auxiliary features are 2 (cash, crypto)\n",
    "        # So, the input to the decoder will be feature_dim + 2\n",
    "        self.decoder = nn.Linear(feature_dim + 2, output_dim) # Corrected input dimension for decoder\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, aux_features):\n",
    "        # src expected shape: (batch_size, sequence_length, feature_dim)\n",
    "        # aux_features expected shape: (batch_size, aux_feature_dim)\n",
    "        src = self.pos_encoder(src) # Add positional encoding\n",
    "        output = self.transformer_encoder(src)\n",
    "        # For RL, typically we might take the output of the last token, or pool.\n",
    "        # Here, we'll take the output of the last token in the sequence (corresponding to current info).\n",
    "        last_token_output = output[:, -1, :] # (batch_size, feature_dim)\n",
    "\n",
    "        # Concatenate the transformer's output with the auxiliary features\n",
    "        combined_features = torch.cat((last_token_output, aux_features), dim=1) # dim=1 to concatenate horizontally\n",
    "\n",
    "        output = self.decoder(combined_features) # Map to Q-values\n",
    "        return output\n",
    "\n",
    "# 3. Environment Simulator\n",
    "class TradingEnv:\n",
    "    \"\"\"\n",
    "    Simulates a simplified crypto trading environment with transaction costs.\n",
    "    Actions: 0 (Hold), 1-4 (Buy percentages), 5-8 (Sell percentages)\n",
    "    \"\"\"\n",
    "    def __init__(self, prices, window_size=WINDOW_SIZE, initial_cash=INITIAL_CASH, fee_percent=TRANSACTION_FEE_PERCENT):\n",
    "        if len(prices) < window_size + 1:\n",
    "            raise ValueError(\"Prices array is too short for the given window_size.\")\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.initial_cash = initial_cash\n",
    "        self.fee_percent = fee_percent\n",
    "\n",
    "        self.min_price_overall = np.min(self.prices)\n",
    "        self.max_price_overall = np.max(self.prices)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "        self.current_step = self.window_size # Start from window_size to have enough history\n",
    "        self.cash = self.initial_cash\n",
    "        self.crypto = 0\n",
    "        self.portfolio_value_history = [self.initial_cash]\n",
    "        self.action_history = []\n",
    "        self.price_history = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the current state: a window of historical prices, with current cash and crypto\n",
    "        integrated at each step of the sequence (or appended as separate features).\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_size:\n",
    "            price_window = np.zeros(self.window_size, dtype=np.float32)\n",
    "        elif self.current_step >= len(self.prices):\n",
    "            price_window = np.zeros(self.window_size, dtype=np.float32)\n",
    "        else:\n",
    "            price_window = np.array(self.prices[self.current_step - self.window_size:self.current_step], dtype=np.float32)\n",
    "\n",
    "        # Normalize cash and crypto\n",
    "        normalized_cash = self.cash / self.initial_cash\n",
    "        # Avoid division by zero and handle very small prices\n",
    "        max_possible_crypto = self.initial_cash / (self.min_price_overall + 1e-9) if self.min_price_overall > 0 else self.initial_cash / 1e-9\n",
    "        normalized_crypto = self.crypto / (max_possible_crypto + 1e-9)\n",
    "\n",
    "        # The state for the environment will still be a single concatenated array.\n",
    "        state = np.concatenate((price_window, [normalized_cash, normalized_crypto]))\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes an action in the environment and returns the next state, reward, and done status.\n",
    "        Actions: 0 (Hold), 1-4 (Buy percentages), 5-8 (Sell percentages)\n",
    "        \"\"\"\n",
    "        if self.current_step >= len(self.prices) - 1:\n",
    "            return self._get_state(), 0, True\n",
    "\n",
    "        price = self.prices[self.current_step]\n",
    "        portfolio_value_before_action = self.cash + self.crypto * price\n",
    "\n",
    "        # Define percentages for buy/sell actions\n",
    "        percentages = [0.25, 0.50, 0.75, 1.00] # 25%, 50%, 75%, 100%\n",
    "\n",
    "        if action >= 1 and action <= 4:  # Buy actions (1 to 4)\n",
    "            buy_percent_idx = action - 1\n",
    "            buy_percentage = percentages[buy_percent_idx]\n",
    "\n",
    "            amount_to_spend = self.cash * buy_percentage\n",
    "\n",
    "            if price * (1 + self.fee_percent) > 0:\n",
    "                amount_to_buy_crypto = amount_to_spend / (price * (1 + self.fee_percent))\n",
    "            else:\n",
    "                amount_to_buy_crypto = 0\n",
    "\n",
    "            if self.cash > 0 and amount_to_buy_crypto > 1e-9: # Small epsilon to avoid tiny trades\n",
    "                self.crypto += amount_to_buy_crypto\n",
    "                self.cash -= (amount_to_buy_crypto * price * (1 + self.fee_percent)) # Deduct spent amount and fees\n",
    "                self.cash = max(0, self.cash) # Ensure cash doesn't go negative\n",
    "\n",
    "        elif action >= 5 and action <= 8:  # Sell actions (5 to 8)\n",
    "            sell_percent_idx = action - 5\n",
    "            sell_percentage = percentages[sell_percent_idx]\n",
    "\n",
    "            amount_to_sell_crypto = self.crypto * sell_percentage\n",
    "\n",
    "            if self.crypto > 1e-9 and amount_to_sell_crypto > 1e-9: # Small epsilon\n",
    "                revenue = amount_to_sell_crypto * price * (1 - self.fee_percent)\n",
    "                self.cash += revenue\n",
    "                self.crypto -= amount_to_sell_crypto # Deduct only the sold crypto amount\n",
    "                self.crypto = max(0, self.crypto) # Ensure crypto doesn't go negative\n",
    "\n",
    "        # Action 0: Hold (do nothing)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "\n",
    "        current_price_for_value = self.prices[self.current_step - 1]\n",
    "        if done:\n",
    "            current_price_for_value = self.prices[-1]\n",
    "\n",
    "        portfolio_value_after_action = self.cash + self.crypto * current_price_for_value\n",
    "        reward = portfolio_value_after_action - portfolio_value_before_action\n",
    "\n",
    "        self.portfolio_value_history.append(portfolio_value_after_action)\n",
    "        self.action_history.append(action)\n",
    "        self.price_history.append(price)\n",
    "\n",
    "        next_state = self._get_state() if not done else np.zeros(self.window_size + 2, dtype=np.float32)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "# Helper function to find the latest checkpoint\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Scans the checkpoint directory and returns the path to the latest checkpoint\n",
    "    and its corresponding episode number.\n",
    "    Returns (None, 0) if no checkpoints are found.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        return None, 0\n",
    "\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"dqn_btc_trader_v3_episode_\") and f.endswith(\".pth\")]\n",
    "\n",
    "    latest_episode = 0\n",
    "    latest_checkpoint_path = None\n",
    "\n",
    "    for f in files:\n",
    "        match = re.search(r'episode_(\\d+)\\.pth', f)\n",
    "        if match:\n",
    "            episode_num = int(match.group(1))\n",
    "            if episode_num > latest_episode:\n",
    "                latest_episode = episode_num\n",
    "                latest_checkpoint_path = os.path.join(checkpoint_dir, f)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint_path} from episode {latest_episode}\")\n",
    "    else:\n",
    "        print(\"üîç No previous checkpoints found. Starting training from scratch.\")\n",
    "\n",
    "    return latest_checkpoint_path, latest_episode\n",
    "\n",
    "\n",
    "# 4. Training Function\n",
    "def train_agent(prices, episodes=1000, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a Transformer-based DQN agent to trade cryptocurrency.\n",
    "    \"\"\"\n",
    "    feature_dim_per_step = 1 # Just the price\n",
    "    aux_features_dim = 2 # Normalized cash and crypto\n",
    "\n",
    "    env = TradingEnv(prices, window_size=window_size)\n",
    "\n",
    "    nhead = 1\n",
    "    num_encoder_layers = 1\n",
    "    dim_feedforward = 64\n",
    "\n",
    "    model = TransformerDQN(feature_dim=feature_dim_per_step, nhead=nhead,\n",
    "                           num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward,\n",
    "                           output_dim=9)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    memory = deque(maxlen=5000)\n",
    "\n",
    "    gamma = 0.95\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.999\n",
    "    batch_size = 64\n",
    "\n",
    "    all_episode_final_profits = []\n",
    "    \n",
    "    # Define the path for the episode metrics CSV\n",
    "    metrics_csv_path = os.path.join(CHECKPOINT_DIR, \"episode_metrics.csv\")\n",
    "\n",
    "    # Load existing metrics if resuming training\n",
    "    if os.path.exists(metrics_csv_path):\n",
    "        episode_metrics_df = pd.read_csv(metrics_csv_path)\n",
    "        episode_metrics = episode_metrics_df.to_dict('records')\n",
    "        print(f\"üìä Loaded existing episode metrics from {metrics_csv_path}\")\n",
    "    else:\n",
    "        episode_metrics = [] # Initialize an empty list if file doesn't exist\n",
    "\n",
    "    latest_checkpoint_path, start_episode = find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        checkpoint = torch.load(latest_checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epsilon = checkpoint['epsilon']\n",
    "        start_episode = checkpoint['episode']\n",
    "        print(f\"Resuming training from episode {start_episode + 1} with epsilon {epsilon:.4f}\")\n",
    "    else:\n",
    "        print(\"Starting training from episode 1.\")\n",
    "        start_episode = 0\n",
    "\n",
    "    print(\"\\nStarting agent training...\")\n",
    "    for ep in range(start_episode, episodes):\n",
    "        state_np = env.reset()\n",
    "        price_sequence = torch.tensor(state_np[:window_size], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        aux_features = torch.tensor(state_np[window_size:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        total_episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(9)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(price_sequence, aux_features)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state_np, reward, done = env.step(action)\n",
    "\n",
    "            next_price_sequence = torch.tensor(next_state_np[:window_size], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "            next_aux_features = torch.tensor(next_state_np[window_size:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            memory.append((price_sequence, aux_features, action, reward, next_price_sequence, next_aux_features, done))\n",
    "\n",
    "            total_episode_reward += reward\n",
    "\n",
    "            price_sequence = next_price_sequence\n",
    "            aux_features = next_aux_features\n",
    "            step_count += 1\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "\n",
    "                price_sequences_batch, aux_features_batch, actions_batch, rewards_batch, next_price_sequences_batch, next_aux_features_batch, dones_batch = zip(*batch)\n",
    "\n",
    "                price_sequences_batch = torch.cat(price_sequences_batch)\n",
    "                aux_features_batch = torch.cat(aux_features_batch)\n",
    "                next_price_sequences_batch = torch.cat(next_price_sequences_batch)\n",
    "                next_aux_features_batch = torch.cat(next_aux_features_batch)\n",
    "\n",
    "                actions_batch = torch.tensor(actions_batch, dtype=torch.long)\n",
    "                rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32)\n",
    "                dones_batch = torch.tensor(dones_batch, dtype=torch.bool)\n",
    "\n",
    "                q_values = model(price_sequences_batch, aux_features_batch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = model(next_price_sequences_batch, next_aux_features_batch).max(1)[0]\n",
    "                    next_q_values[dones_batch] = 0.0\n",
    "\n",
    "                targets = q_values.clone()\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    targets[i, actions_batch[i]] = rewards_batch[i] + (gamma * next_q_values[i])\n",
    "\n",
    "                loss = loss_fn(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        final_profit_this_episode = env.portfolio_value_history[-1] - env.initial_cash\n",
    "        all_episode_final_profits.append(final_profit_this_episode)\n",
    "\n",
    "        action_counts = Counter(env.action_history)\n",
    "        buys = sum(action_counts.get(i, 0) for i in range(1, 5))\n",
    "        sells = sum(action_counts.get(i, 0) for i in range(5, 9))\n",
    "        holds = action_counts.get(0, 0)\n",
    "\n",
    "        print(f\"Episode {ep+1}/{episodes}, Final Profit: ${final_profit_this_episode:.2f}, Epsilon: {epsilon:.4f}, Steps: {step_count}, Buys: {buys}, Sells: {sells}, Holds: {holds}\")\n",
    "\n",
    "        # --- IMPORTANT CHANGE STARTS HERE ---\n",
    "        # Create a DataFrame for the current episode's metrics\n",
    "        current_episode_df = pd.DataFrame([{\n",
    "            'episode': ep + 1,\n",
    "            'final_profit': int(final_profit_this_episode),\n",
    "            'buys': buys,\n",
    "            'sells': sells,\n",
    "            'holds': holds\n",
    "        }])\n",
    "\n",
    "        # Append to CSV\n",
    "        if not os.path.exists(metrics_csv_path):\n",
    "            current_episode_df.to_csv(metrics_csv_path, index=False)\n",
    "        else:\n",
    "            current_episode_df.to_csv(metrics_csv_path, mode='a', header=False, index=False)\n",
    "        # --- IMPORTANT CHANGE ENDS HERE ---\n",
    "\n",
    "        if (ep + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"dqn_btc_trader_v3_episode_{ep + 1}.pth\")\n",
    "            torch.save({\n",
    "                'episode': ep + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epsilon': epsilon\n",
    "            }, checkpoint_path)\n",
    "            print(f\"--- Model weights saved to {checkpoint_path} ---\")\n",
    "\n",
    "    final_model_save_path = os.path.join(CHECKPOINT_DIR, \"dqn_btc_trader_v3_final.pth\")\n",
    "    torch.save({\n",
    "        'episode': episodes,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epsilon': epsilon\n",
    "    }, final_model_save_path)\n",
    "    print(f\"\\n‚úÖ Training complete. Final model saved to {final_model_save_path}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_episode + 1, episodes + 1), all_episode_final_profits)\n",
    "    plt.title(\"Episode Final Net Profit Over Time (from resumed point)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Final Net Profit ($)\")\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', label='Break-even')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Testing trained agent's performance ---\")\n",
    "    test_env = TradingEnv(prices, window_size=window_size)\n",
    "    model.eval()\n",
    "    test_state_np = test_env.reset()\n",
    "    test_price_sequence = torch.tensor(test_state_np[:window_size], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "    test_aux_features = torch.tensor(test_state_np[window_size:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "    test_total_profit = 0\n",
    "    test_done = False\n",
    "\n",
    "    while not test_done:\n",
    "        with torch.no_grad():\n",
    "            test_q_values = model(test_price_sequence, test_aux_features)\n",
    "            action = torch.argmax(test_q_values).item()\n",
    "\n",
    "        next_state_np, reward, test_done = test_env.step(action)\n",
    "        test_price_sequence = torch.tensor(next_state_np[:window_size], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        test_aux_features = torch.tensor(next_state_np[window_size:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        test_total_profit = test_env.portfolio_value_history[-1] - test_env.initial_cash\n",
    "\n",
    "    test_action_counts = Counter(test_env.action_history)\n",
    "    test_buys = sum(test_action_counts.get(i, 0) for i in range(1, 5))\n",
    "    test_sells = sum(test_action_counts.get(i, 0) for i in range(5, 9))\n",
    "    test_holds = test_action_counts.get(0, 0)\n",
    "\n",
    "    print(f\"Test Run Final Profit: ${test_total_profit:.2f}, Buys: {test_buys}, Sells: {test_sells}, Holds: {test_holds}\")\n",
    "\n",
    "    history_df = pd.DataFrame({\n",
    "        'step': range(len(test_env.portfolio_value_history)),\n",
    "        'portfolio_value': test_env.portfolio_value_history,\n",
    "        'action': [None] * (test_env.window_size) + test_env.action_history if test_env.action_history else []\n",
    "    })\n",
    "\n",
    "    actual_prices_for_plot = []\n",
    "    if len(test_env.prices) >= test_env.window_size:\n",
    "        actual_prices_for_plot.append(test_env.prices[test_env.window_size - 1])\n",
    "\n",
    "    for i in range(len(test_env.action_history)):\n",
    "        if (test_env.window_size + i) < len(test_env.prices):\n",
    "            actual_prices_for_plot.append(test_env.prices[test_env.window_size + i])\n",
    "        else:\n",
    "            actual_prices_for_plot.append(test_env.prices[-1])\n",
    "\n",
    "    history_df['price_at_step'] = actual_prices_for_plot[:len(history_df['portfolio_value'])]\n",
    "\n",
    "\n",
    "    if not history_df.empty:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.plot(history_df['step'], history_df['portfolio_value'], label='Portfolio Value ($)', color='blue')\n",
    "\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(history_df['step'], history_df['price_at_step'], label='BTC Price ($)', color='orange', alpha=0.7, linestyle='--')\n",
    "        ax2.set_ylabel(\"BTC Price ($)\")\n",
    "\n",
    "        if 'action' in history_df.columns and not history_df['action'].isnull().all():\n",
    "            buy_actions_to_plot = [1, 2, 3, 4]\n",
    "            sell_actions_to_plot = [5, 6, 7, 8]\n",
    "\n",
    "            buys_plot = history_df[history_df['action'].isin(buy_actions_to_plot)]\n",
    "            sells_plot = history_df[history_df['action'].isin(sell_actions_to_plot)]\n",
    "\n",
    "            if not buys_plot.empty:\n",
    "                plt.scatter(buys_plot['step'], history_df.loc[buys_plot.index, 'price_at_step'], marker='^', color='green', s=100, label='Buy', alpha=1, zorder=5)\n",
    "            if not sells_plot.empty:\n",
    "                plt.scatter(sells_plot['step'], history_df.loc[sells_plot.index, 'price_at_step'], marker='v', color='red', s=100, label='Sell', alpha=1, zorder=5)\n",
    "\n",
    "        plt.title(\"Trained Agent Performance (Test Run)\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Portfolio Value ($)\")\n",
    "        plt.grid(True)\n",
    "        lines, labels = plt.gca().get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Test history is empty, cannot plot test performance.\")\n",
    "        \n",
    "# --- New function for getting daily recommendation ---\n",
    "def get_daily_recommendation(window_size, initial_cash, overall_min_price, overall_max_price):\n",
    "    \"\"\"\n",
    "    Loads the latest trained model and provides a trading recommendation\n",
    "    based on the most recent price data and user's current portfolio.\n",
    "    It now uses the enhanced get_price_data function to ensure up-to-date prices.\n",
    "    \"\"\"\n",
    "    feature_dim_per_step = 1 # Just the price\n",
    "    aux_features_dim = 2 # Normalized cash and crypto\n",
    "\n",
    "    # Initialize model\n",
    "    nhead = 1\n",
    "    num_encoder_layers = 1\n",
    "    dim_feedforward = 64\n",
    "    model = TransformerDQN(feature_dim=feature_dim_per_step, nhead=nhead,\n",
    "                           num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward,\n",
    "                           output_dim=9)\n",
    "\n",
    "    # Find and load the latest checkpoint\n",
    "    latest_checkpoint_path, _ = find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "    if not latest_checkpoint_path:\n",
    "        print(\"‚ùå No trained model found. Please train the agent first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Loading model from {latest_checkpoint_path} for recommendation ---\")\n",
    "    # Load with weights_only=True\n",
    "    checkpoint = torch.load(latest_checkpoint_path, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # --- Fetching latest price data for inference using the enhanced get_price_data ---\n",
    "    print(\"üåê Getting latest BTC data for recommendation (from CSV, then online if needed)...\")\n",
    "    try:\n",
    "        # This call will now ensure the CSV is updated and return the complete, recent price history\n",
    "        all_recent_prices = get_price_data()\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"‚ùå Cannot get a recommendation without sufficient recent price data.\")\n",
    "        return\n",
    "\n",
    "    if len(all_recent_prices) < window_size:\n",
    "        print(f\"Error: Not enough historical data ({len(all_recent_prices)} points) to form a window of size {window_size}.\")\n",
    "        print(\"Please ensure your data source provides sufficient data.\")\n",
    "        return\n",
    "\n",
    "    # Extract the last `window_size` prices for the sequence input\n",
    "    latest_price_window = all_recent_prices[-window_size:]\n",
    "    current_btc_price = all_recent_prices[-1] # Today's closing price or latest available price\n",
    "\n",
    "    print(f\"Latest price data for prediction window (last {window_size} days):\")\n",
    "    print(latest_price_window)\n",
    "    print(f\"Current BTC price (most recent): ${current_btc_price:.2f}\")\n",
    "\n",
    "    # Prompt user for current cash and crypto holdings\n",
    "    try:\n",
    "        current_cash = float(input(\"\\nEnter your current cash balance (e.g., 10000): \"))\n",
    "        current_crypto = float(input(\"Enter your current Bitcoin (BTC) quantity (e.g., 0.5): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter numerical values for cash and crypto.\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare input tensors ---\n",
    "    # Scale price window using the overall min/max passed as arguments from training data\n",
    "    scaled_price_window = (latest_price_window - overall_min_price) / \\\n",
    "                          (overall_max_price - overall_min_price + 1e-9)\n",
    "\n",
    "    price_sequence_tensor = torch.tensor(scaled_price_window, dtype=torch.float32).unsqueeze(0).unsqueeze(-1) # (1, seq_len, 1)\n",
    "\n",
    "    # Normalize auxiliary features\n",
    "    normalized_cash = current_cash / initial_cash\n",
    "    max_possible_crypto = initial_cash / (overall_min_price + 1e-9)\n",
    "    normalized_crypto = current_crypto / (max_possible_crypto + 1e-9)\n",
    "    aux_features_tensor = torch.tensor([normalized_cash, normalized_crypto], dtype=torch.float32).unsqueeze(0) # (1, 2)\n",
    "\n",
    "    print(f\"Normalized cash: {normalized_cash:.4f}, Normalized crypto: {normalized_crypto:.4f}\")\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        q_values = model(price_sequence_tensor, aux_features_tensor)\n",
    "        predicted_action_index = torch.argmax(q_values).item()\n",
    "\n",
    "    # Map action index to human-readable action\n",
    "    action_mapping = {\n",
    "        0: 'Hold (Do nothing)',\n",
    "        1: 'Buy 25% of available cash',\n",
    "        2: 'Buy 50% of available cash',\n",
    "        3: 'Buy 75% of available cash',\n",
    "        4: 'Buy 100% of available cash',\n",
    "        5: 'Sell 25% of current crypto holdings',\n",
    "        6: 'Sell 50% of current crypto holdings',\n",
    "        7: 'Sell 75% of current crypto holdings',\n",
    "        8: 'Sell 100% of current crypto holdings'\n",
    "    }\n",
    "    recommendation = action_mapping.get(predicted_action_index, \"Unknown Action\")\n",
    "\n",
    "    print(\"\\n--- Recommendation for Tomorrow ---\")\n",
    "    print(f\"Based on the trained agent, the recommended action is: \\nüëâ {recommendation}\")\n",
    "    print(f\"Current portfolio value (estimated): ${current_cash + current_crypto * current_btc_price:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba657fe-8e4b-4afd-9735-97af7db1e0a1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec620b-e06c-45a9-b790-65e86b7f54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # train_agent still needs the full historical data\n",
    "    btc_prices_for_training = get_price_data()\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"Exiting. Please ensure your CSV has a 'Close' column or check the data source.\")\n",
    "    exit()\n",
    "\n",
    "if len(btc_prices_for_training) < WINDOW_SIZE + 2:\n",
    "    print(f\"Error: Not enough historical data ({len(btc_prices_for_training)} points) for window size {WINDOW_SIZE} with additional features.\")\n",
    "    print(\"Please ensure your CSV file or Yahoo Finance download provides sufficient data.\")\n",
    "    exit()\n",
    "\n",
    "# You can comment out or reduce episodes for quick testing after initial training\n",
    "train_agent(btc_prices_for_training, episodes=2000, window_size=WINDOW_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f1800-3f4a-4478-942f-9252b5dcfe33",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea335228-a56c-47b6-b7d4-a7130ad55e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cbica/home/dadashkj/.conda/envs/tf-gpu-jk/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading BTC data from btc_prices.csv\n",
      "Loaded data shape: (2715, 1)\n",
      "‚úÖ Found latest checkpoint: amexyz/checkpoints_120/dqn_btc_trader_v3_episode_600.pth from episode 600\n",
      "\n",
      "--- Loading model from amexyz/checkpoints_120/dqn_btc_trader_v3_episode_600.pth for recommendation ---\n",
      "üåê Getting latest BTC data for recommendation (from CSV, then online if needed)...\n",
      "üìÅ Loading BTC data from btc_prices.csv\n",
      "Loaded data shape: (2715, 1)\n",
      "Latest price data for prediction window (last 120 days):\n",
      "[ 96482.45   96500.09   97437.555  95747.43   97885.86   96623.87\n",
      "  97508.97   97580.35   96175.03   95773.38   95539.55   96635.61\n",
      "  98333.94   96125.55   96577.76   96273.92   91418.17   88736.17\n",
      "  84347.02   84704.23   84373.01   86031.914  94248.35   86065.67\n",
      "  87222.195  90623.56   89961.73   86742.67   86154.59   80601.04\n",
      "  78532.     82862.21   83722.36   81066.7    83969.1    84343.11\n",
      "  82579.69   84075.69   82718.5    86854.23   84167.195  84043.24\n",
      "  83832.484  86054.375  87498.914  87471.7    86900.88   87177.1\n",
      "  84353.15   82597.586  82334.52   82548.914  85169.17   82485.71\n",
      "  83102.83   83843.805  83504.8    78214.484  79235.336  76271.95\n",
      "  82573.95   79626.14   83404.836  85287.11   83684.98   84542.39\n",
      "  83668.99   84033.87   84895.75   84450.805  85063.414  85174.305\n",
      "  87518.91   93441.89   93699.11   93943.8    94720.5    94646.93\n",
      "  93754.84   94978.75   94284.79   94207.31   96492.336  96910.07\n",
      "  95891.8    94315.98   94748.055  96802.48   97032.32  103241.46\n",
      " 102970.85  104696.33  104106.36  102812.95  104169.81  103539.414\n",
      " 103744.64  103489.29  103191.086 106446.01  105606.18  106791.086\n",
      " 109678.08  111673.28  107287.8   107791.16  109035.39  109440.37\n",
      " 108994.64  107802.33  105641.76  103998.57  104638.09  105652.1\n",
      " 105881.53  105432.47  104731.984 101575.95  104390.34  105615.625]\n",
      "Current BTC price (most recent): $105615.62\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your current cash balance (e.g., 10000):  100\n",
      "Enter your current Bitcoin (BTC) quantity (e.g., 0.5):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized cash: 0.0100, Normalized crypto: 0.0000\n",
      "\n",
      "--- Recommendation for Tomorrow ---\n",
      "Based on the trained agent, the recommended action is: \n",
      "üëâ Buy 100% of available cash\n",
      "Current portfolio value (estimated): $100.00\n"
     ]
    }
   ],
   "source": [
    "btc_prices_for_training = get_price_data()\n",
    "overall_min_price_training = np.min(btc_prices_for_training)\n",
    "overall_max_price_training = np.max(btc_prices_for_training)\n",
    "\n",
    "get_daily_recommendation(WINDOW_SIZE, INITIAL_CASH,overall_min_price_training, overall_max_price_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08dd377-9aaa-4640-911b-3099a9ca071e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
